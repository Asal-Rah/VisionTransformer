# VisionTransformer
In this project a vision transformer was implemeted and trained on CIFAR-10 images for classification.
Ar first, using the multi-head attention mechanism a vision transformer was implemented. Then, cifat-10 dataset was loaded and our tranformer was trained on this dataset with the set hyperparameters, including 10 epochs, batch size of 64, and learning rate of 0.001. During the whole training process, training loss was being monitored to prevent any issues regarding over/underfit of our model.  Later, for evaluation the accuracy metric has been used, which was more than 51% in the first try.
In the initial training process, training loss wasn't declining steadily and there were cases that I witnessed it hovering around an amount. In order to fix this problem, I tried tuning the hyperparameters. Hence, I set the learning rate to 0.0001 and batch size to 256. After training our tranformer again with these new hyperparameters, the result got better but there was still room for improvements. That is why I used L2 regularization technique, which prevents weights from becoming too large by controling the amount of loss. At last, the accuracy increased by almost 6%.
For the last part of this project, I tried using pre-trained models to see whether they give better results or not. VGG-19  model was used on this dataset, with the batch size of 128 and 5 number of epochs. As you can see the amount of loss during each epochs is not decreasing enough. Also during each of the epochs the loss of doesn't reduce in a stable manner.
